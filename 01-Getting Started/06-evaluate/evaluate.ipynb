{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate使用指南"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查看支持的评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo:https://huggingface.co/tasks  huggingface的文档里的tasks可以查看支持哪些任务，每个点进去最右下就是这个任务用到的评估指标\n",
    "\n",
    "# 在2024-01-11的测试中，list_evaluation_modules无法完全显示支持的评估函数，但不影响使用\n",
    "# 完成的评估函数可以在 https://huggingface.co/evaluate-metric 中查看\n",
    "# 支持这些评估函数\n",
    "evaluate.list_evaluation_modules(with_details=True) # with_details=True可以看用于哪些任务"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "[{'name': 'lvwerra/test', 'type': 'metric', 'community': True, 'likes': 0},\n",
    " {'name': 'angelina-wang/directional_bias_amplification',\n",
    "  'type': 'metric',\n",
    "  'community': True,\n",
    "  'likes': 6},\n",
    " {'name': 'cpllab/syntaxgym', 'type': 'metric', 'community': True, 'likes': 1},\n",
    " {'name': 'lvwerra/bary_score',\n",
    "  'type': 'metric',\n",
    "  'community': True,\n",
    "  'likes': 1},]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate.list_evaluation_modules(\n",
    "  module_type=\"comparison\",\n",
    "  include_community=False,\n",
    "  with_details=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo:查看文档\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "EvaluationModule(name: \"accuracy\", module_type: \"metric\", features: {'predictions': Value('int32'), 'references': Value('int32')}, usage: \"\"\"\n",
    "Args:\n",
    "    predictions (`list` of `int`): Predicted labels.        # 预测标签\n",
    "    references (`list` of `int`): Ground truth labels.      # 真实标签\n",
    "    normalize (`boolean`): If set to False, returns the number of correctly classified samples. Otherwise, returns the fraction of correctly classified samples. Defaults to True.      # 是否进行归1化\n",
    "    sample_weight (`list` of `float`): Sample weights Defaults to None.\n",
    "\n",
    "Returns:\n",
    "    accuracy (`float` or `int`): Accuracy score. Minimum possible value is 0. Maximum possible value is 1.0, or the number of examples input, if `normalize` is set to `True`.. A higher score means higher accuracy.\n",
    "\n",
    "Examples:\n",
    "\n",
    "    Example 1-A simple example\n",
    "        >>> accuracy_metric = evaluate.load(\"accuracy\")\n",
    "        >>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0])\n",
    "        >>> print(results)\n",
    "        {'accuracy': 0.5}\n",
    "\n",
    "    Example 2-The same as Example 1, except with `normalize` set to `False`.\n",
    "        >>> accuracy_metric = evaluate.load(\"accuracy\")\n",
    "        >>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], normalize=False)  # 不归一化，返回正确个数\n",
    "        >>> print(results)\n",
    "        {'accuracy': 3.0}\n",
    "\n",
    "    Example 3-The same as Example 1, except with `sample_weight` set.\n",
    "        >>> accuracy_metric = evaluate.load(\"accuracy\")\n",
    "        >>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], sample_weight=[0.5, 2, 0.7, 0.5, 9, 0.4])\n",
    "        >>> print(results)\n",
    "        {'accuracy': 0.8778625954198473}\n",
    "\"\"\", stored examples: 0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查看函数说明"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo:简单介绍\n",
    "print(accuracy.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo:查看输入格式\n",
    "print(accuracy.inputs_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估指标计算——全局计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一次性给出所有的预测值和真实值进行计算\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "results = accuracy.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0])\n",
    "results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估指标计算——迭代计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "for ref, pred in zip([0,1,0,1], [1,0,0,1]):\n",
    "    accuracy.add(references=ref, predictions=pred)\n",
    "accuracy.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分批给出，add进accuracy，最后再一次性compute\n",
    "# todo:对于batch,一次添加一整个batch进去\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "for refs, preds in zip([[0,1],[0,1]], [[1,0],[0,1]]):\n",
    "    accuracy.add_batch(references=refs, predictions=preds)\n",
    "accuracy.compute()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多个评估指标计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"recall\", \"precision\"]) # 同时用多个评估函数\n",
    "clf_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_metrics.compute(predictions=[0, 1, 0], references=[0, 1, 1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估结果对比可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate.visualization import radar_plot   # 目前只支持雷达图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "   {\"accuracy\": 0.99, \"precision\": 0.8, \"f1\": 0.95, \"latency_in_seconds\": 33.6},\n",
    "   {\"accuracy\": 0.98, \"precision\": 0.87, \"f1\": 0.91, \"latency_in_seconds\": 11.2},\n",
    "   {\"accuracy\": 0.98, \"precision\": 0.78, \"f1\": 0.88, \"latency_in_seconds\": 87.6}, \n",
    "   {\"accuracy\": 0.88, \"precision\": 0.78, \"f1\": 0.81, \"latency_in_seconds\": 101.6}\n",
    "   ]\n",
    "model_names = [\"Model 1\", \"Model 2\", \"Model 3\", \"Model 4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = radar_plot(data=data, model_names=model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
