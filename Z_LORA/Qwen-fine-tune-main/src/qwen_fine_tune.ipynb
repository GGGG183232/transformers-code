{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install peft\n!pip install modelscope","metadata":{"execution":{"iopub.status.busy":"2024-06-17T04:28:47.766791Z","iopub.execute_input":"2024-06-17T04:28:47.767437Z","iopub.status.idle":"2024-06-17T04:29:27.967343Z","shell.execute_reply.started":"2024-06-17T04:28:47.767405Z","shell.execute_reply":"2024-06-17T04:29:27.966395Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting peft\n  Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.1)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.1.2)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.41.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.30.1)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.3)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.23.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.3.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.12.25)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.19.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.2.2)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.11.1-py3-none-any.whl (251 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.11.1\nCollecting modelscope\n  Downloading modelscope-1.15.0-py3-none-any.whl.metadata (33 kB)\nCollecting addict (from modelscope)\n  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\nRequirement already satisfied: attrs in /opt/conda/lib/python3.10/site-packages (from modelscope) (23.2.0)\nCollecting datasets<2.19.0,>=2.16.0 (from modelscope)\n  Downloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\nCollecting einops (from modelscope)\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: filelock>=3.3.0 in /opt/conda/lib/python3.10/site-packages (from modelscope) (3.13.1)\nRequirement already satisfied: gast>=0.2.2 in /opt/conda/lib/python3.10/site-packages (from modelscope) (0.5.4)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from modelscope) (0.23.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from modelscope) (1.26.4)\nCollecting oss2 (from modelscope)\n  Downloading oss2-2.18.6.tar.gz (283 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.8/283.8 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from modelscope) (2.2.1)\nRequirement already satisfied: Pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from modelscope) (9.5.0)\nRequirement already satisfied: pyarrow!=9.0.0,>=6.0.0 in /opt/conda/lib/python3.10/site-packages (from modelscope) (14.0.2)\nRequirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.10/site-packages (from modelscope) (2.9.0.post0)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from modelscope) (6.0.1)\nRequirement already satisfied: requests>=2.25 in /opt/conda/lib/python3.10/site-packages (from modelscope) (2.32.3)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from modelscope) (1.11.4)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from modelscope) (69.0.3)\nRequirement already satisfied: simplejson>=3.3.0 in /opt/conda/lib/python3.10/site-packages (from modelscope) (3.19.2)\nRequirement already satisfied: sortedcontainers>=1.5.9 in /opt/conda/lib/python3.10/site-packages (from modelscope) (2.4.0)\nRequirement already satisfied: tqdm>=4.64.0 in /opt/conda/lib/python3.10/site-packages (from modelscope) (4.66.4)\nRequirement already satisfied: urllib3>=1.26 in /opt/conda/lib/python3.10/site-packages (from modelscope) (1.26.18)\nRequirement already satisfied: yapf in /opt/conda/lib/python3.10/site-packages (from modelscope) (0.40.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets<2.19.0,>=2.16.0->modelscope) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets<2.19.0,>=2.16.0->modelscope) (0.3.8)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets<2.19.0,>=2.16.0->modelscope) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets<2.19.0,>=2.16.0->modelscope) (0.70.16)\nCollecting fsspec<=2024.2.0,>=2023.1.0 (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets<2.19.0,>=2.16.0->modelscope)\n  Downloading fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets<2.19.0,>=2.16.0->modelscope) (3.9.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets<2.19.0,>=2.16.0->modelscope) (21.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->modelscope) (4.9.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.1->modelscope) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.25->modelscope) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.25->modelscope) (3.6)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.25->modelscope) (2024.2.2)\nRequirement already satisfied: crcmod>=1.7 in /opt/conda/lib/python3.10/site-packages (from oss2->modelscope) (1.7)\nRequirement already satisfied: pycryptodome>=3.4.7 in /opt/conda/lib/python3.10/site-packages (from oss2->modelscope) (3.20.0)\nCollecting aliyun-python-sdk-kms>=2.4.1 (from oss2->modelscope)\n  Downloading aliyun_python_sdk_kms-2.16.3-py2.py3-none-any.whl.metadata (1.5 kB)\nCollecting aliyun-python-sdk-core>=2.13.12 (from oss2->modelscope)\n  Downloading aliyun-python-sdk-core-2.15.1.tar.gz (443 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.1/443.1 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->modelscope) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->modelscope) (2023.4)\nRequirement already satisfied: importlib-metadata>=6.6.0 in /opt/conda/lib/python3.10/site-packages (from yapf->modelscope) (6.11.0)\nRequirement already satisfied: platformdirs>=3.5.1 in /opt/conda/lib/python3.10/site-packages (from yapf->modelscope) (3.11.0)\nRequirement already satisfied: tomli>=2.0.1 in /opt/conda/lib/python3.10/site-packages (from yapf->modelscope) (2.0.1)\nCollecting jmespath<1.0.0,>=0.9.3 (from aliyun-python-sdk-core>=2.13.12->oss2->modelscope)\n  Downloading jmespath-0.10.0-py2.py3-none-any.whl.metadata (8.0 kB)\nRequirement already satisfied: cryptography>=2.6.0 in /opt/conda/lib/python3.10/site-packages (from aliyun-python-sdk-core>=2.13.12->oss2->modelscope) (41.0.7)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets<2.19.0,>=2.16.0->modelscope) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets<2.19.0,>=2.16.0->modelscope) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets<2.19.0,>=2.16.0->modelscope) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets<2.19.0,>=2.16.0->modelscope) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets<2.19.0,>=2.16.0->modelscope) (4.0.3)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata>=6.6.0->yapf->modelscope) (3.17.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets<2.19.0,>=2.16.0->modelscope) (3.1.1)\nRequirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.10/site-packages (from cryptography>=2.6.0->aliyun-python-sdk-core>=2.13.12->oss2->modelscope) (1.16.0)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=2.6.0->aliyun-python-sdk-core>=2.13.12->oss2->modelscope) (2.21)\nDownloading modelscope-1.15.0-py3-none-any.whl (5.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading datasets-2.18.0-py3-none-any.whl (510 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading addict-2.4.0-py3-none-any.whl (3.8 kB)\nDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading aliyun_python_sdk_kms-2.16.3-py2.py3-none-any.whl (98 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.1/98.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\nBuilding wheels for collected packages: oss2, aliyun-python-sdk-core\n  Building wheel for oss2 (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for oss2: filename=oss2-2.18.6-py3-none-any.whl size=118354 sha256=e80409e339c36d9181d81bd7a879d18e8639f7b8e1884e9a9eb6c62b8bbbe118\n  Stored in directory: /root/.cache/pip/wheels/e9/1c/df/6256a3d22097f6e1a30edd892de172054fd27875e0a349b4a4\n  Building wheel for aliyun-python-sdk-core (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for aliyun-python-sdk-core: filename=aliyun_python_sdk_core-2.15.1-py3-none-any.whl size=535325 sha256=88e547d83856d9d8bed25195eba0f81d151adefa67cec91d195f041b0f21e173\n  Stored in directory: /root/.cache/pip/wheels/69/4b/8e/0a28e00f4cf43b273c18cce083804738d41013e017da922ce4\nSuccessfully built oss2 aliyun-python-sdk-core\nInstalling collected packages: addict, jmespath, fsspec, einops, aliyun-python-sdk-core, datasets, aliyun-python-sdk-kms, oss2, modelscope\n  Attempting uninstall: jmespath\n    Found existing installation: jmespath 1.0.1\n    Uninstalling jmespath-1.0.1:\n      Successfully uninstalled jmespath-1.0.1\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2024.3.1\n    Uninstalling fsspec-2024.3.1:\n      Successfully uninstalled fsspec-2024.3.1\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.19.2\n    Uninstalling datasets-2.19.2:\n      Successfully uninstalled datasets-2.19.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.4.1 requires cubinlinker, which is not installed.\ncudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.4.1 requires ptxcompiler, which is not installed.\ncuml 24.4.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\nboto3 1.26.100 requires botocore<1.30.0,>=1.29.100, but you have botocore 1.34.106 which is incompatible.\ncudf 24.4.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\ndistributed 2024.1.1 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\ngcsfs 2024.3.1 requires fsspec==2024.3.1, but you have fsspec 2024.2.0 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask-expr==0.4.0, but you have dask-expr 1.1.2 which is incompatible.\ns3fs 2024.3.1 requires fsspec==2024.3.1, but you have fsspec 2024.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed addict-2.4.0 aliyun-python-sdk-core-2.15.1 aliyun-python-sdk-kms-2.16.3 datasets-2.18.0 einops-0.8.0 fsspec-2024.2.0 jmespath-0.10.0 modelscope-1.15.0 oss2-2.18.6\n","output_type":"stream"}]},{"cell_type":"code","source":"#查看GPU信息\nimport subprocess\n# 执行nvidia-smi命令\nresult = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n# 打印输出\nprint(result.stdout)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T04:33:00.996075Z","iopub.execute_input":"2024-06-17T04:33:00.996885Z","iopub.status.idle":"2024-06-17T04:33:01.047787Z","shell.execute_reply.started":"2024-06-17T04:33:00.996826Z","shell.execute_reply":"2024-06-17T04:33:01.046881Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Mon Jun 17 04:33:01 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla P100-PCIE-16GB           Off | 00000000:00:04.0 Off |                    0 |\n| N/A   37C    P0              26W / 250W |      0MiB / 16384MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|  No running processes found                                                           |\n+---------------------------------------------------------------------------------------+\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 【数据处理】","metadata":{}},{"cell_type":"code","source":"# import datasets\n# dataset = datasets.load_dataset(\"yelp_review_full\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import modelscope\nfrom modelscope.msdatasets import MsDataset\n#【下载数据集】\nHC3=MsDataset.load('simpleai/HC3-Chinese', subset_name='baike', split='train') #调用HC3数据集\ndataset=HC3.to_hf_dataset() #将MsDataset转换成huggingface dataset格式，方便后续处理\nprint(\"【数据集下载完成】\")","metadata":{"execution":{"iopub.status.busy":"2024-06-17T04:33:03.910565Z","iopub.execute_input":"2024-06-17T04:33:03.911282Z","iopub.status.idle":"2024-06-17T04:33:24.181989Z","shell.execute_reply.started":"2024-06-17T04:33:03.911252Z","shell.execute_reply":"2024-06-17T04:33:24.181054Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"2024-06-17 04:33:07,896 - modelscope - INFO - PyTorch version 2.1.2 Found.\n2024-06-17 04:33:07,902 - modelscope - INFO - TensorFlow version 2.15.0 Found.\n2024-06-17 04:33:07,903 - modelscope - INFO - Loading ast index from /root/.cache/modelscope/ast_indexer\n2024-06-17 04:33:07,903 - modelscope - INFO - No valid ast index found from /root/.cache/modelscope/ast_indexer, generating ast index from prebuilt!\n2024-06-17 04:33:07,985 - modelscope - INFO - Loading done! Current index file version is 1.15.0, with md5 cf8dafbb4e308ed48127fabe0ef1ceec and a total number of 980 components indexed\n/opt/conda/lib/python3.10/site-packages/datasets/load.py:2524: FutureWarning: 'ignore_verifications' was deprecated in favor of 'verification_mode' in version 2.9.1 and will be removed in 3.0.0.\nYou can remove this warning by passing 'verification_mode=no_checks' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/datasets/load.py:926: FutureWarning: The repository for HC3-Chinese contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /root/.cache/modelscope/hub/datasets/simpleai/HC3-Chinese/master/meta/HC3-Chinese.py\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8962d638add4f7c9945d57c473d27c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fb3172fa78049c68514bcf13508b047"}},"metadata":{}},{"name":"stdout","text":"【数据集下载完成】\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import Dataset\n#【调整数据集格式】\ndef data_init(dataset):\n    ds=[]\n    cnt=dataset.num_rows\n    for i in range(cnt):\n        example=dataset[i]\n        ds.append({\"label\":0,\"text\":example[\"human_answers\"][0]})\n        ds.append({\"label\":1,\"text\":example[\"chatgpt_answers\"][0]})\n    return Dataset.from_list(ds)\n\ndataset=data_init(dataset) # 调整数据集内容\nprint(dataset)\ndataset=dataset.shuffle(seed=233).select(range(5000)) #随机选一部分\n\n#数据集划分 train:val:test=8:1:1\ndata_=dataset.train_test_split(train_size=0.8,seed=233) #数据集划分\ndata_train=data_[\"train\"]\ndata__=data_[\"test\"].train_test_split(train_size=0.5,seed=233)\ndata_val=data__[\"train\"]\ndata_test=data__[\"test\"]\n\nprint(\"【data_train】\",data_train)\nprint(\"【data_val】\",data_val)\nprint(\"【data_test】\",data_test)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T04:33:34.584724Z","iopub.execute_input":"2024-06-17T04:33:34.585363Z","iopub.status.idle":"2024-06-17T04:33:35.083295Z","shell.execute_reply.started":"2024-06-17T04:33:34.585330Z","shell.execute_reply":"2024-06-17T04:33:35.082379Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['label', 'text'],\n    num_rows: 9234\n})\n【data_train】 Dataset({\n    features: ['label', 'text'],\n    num_rows: 4000\n})\n【data_val】 Dataset({\n    features: ['label', 'text'],\n    num_rows: 500\n})\n【data_test】 Dataset({\n    features: ['label', 'text'],\n    num_rows: 500\n})\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 【模型】","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer,AutoModelForSequenceClassification,TrainingArguments,Trainer,DataCollatorWithPadding\n\n#【加载分词器】\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-0.5B\") #(\"bert-base-cased\")\ntokenizer.pad_token_id = tokenizer.eos_token_id #Qwen特性，需要指定一下pad_token_id\nprint(tokenizer)\n# print(\"【pad_token_id】\",tokenizer.pad_token_id)\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"],padding=\"max_length\",truncation=True,max_length=512)\n\ntoken_train=data_train.map(tokenize_function, batched=True)\ntoken_val=data_val.map(tokenize_function, batched=True)\nprint(\"【token_train[0]】\",token_train[0])\n\ntrain_dataset = token_train\neval_dataset = token_val","metadata":{"execution":{"iopub.status.busy":"2024-06-17T04:33:38.397054Z","iopub.execute_input":"2024-06-17T04:33:38.397691Z","iopub.status.idle":"2024-06-17T04:33:55.072749Z","shell.execute_reply.started":"2024-06-17T04:33:38.397659Z","shell.execute_reply":"2024-06-17T04:33:55.071859Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"2024-06-17 04:33:40.508356: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-17 04:33:40.508488: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-17 04:33:40.630987: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9bb136cc1954e5db55456ead79c6a72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8c3ca07dec342b8b78c205bfda30a2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37f5fd067c4c40d2b642c6b0322caf4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3deb0d591ce4d908007270c3c854fe9"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"name":"stdout","text":"Qwen2TokenizerFast(name_or_path='Qwen/Qwen1.5-0.5B', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|endoftext|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03a6ab6ea4574eaeb2013890c58a7275"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc019884950f405dbbe6898a8247cb35"}},"metadata":{}},{"name":"stdout","text":"【token_train[0]】 {'label': 0, 'text': '网络节点是指一台电脑或其他设备与一个有独立地址和具有传送或接收数据功能的网络相连。节点可以是工作站、客户、网络用户或个人计算机，还可以是服务器、打印机和其他网络连接的设备。每一个工作站﹑服务器、终端设备、网络设备，即拥有自己唯一网络地址的设备都是网络节点。整个网络就是由这许许多多的网络节点组成的，把许多的网络节点用通信线路连接起来，形成一定的几何关系，这就是计算机网络拓扑。', 'input_ids': [71356, 92374, 104442, 106621, 104145, 105994, 101044, 57218, 46944, 18830, 102024, 46477, 33108, 100629, 112523, 57191, 106585, 20074, 98380, 9370, 71356, 111060, 1773, 92374, 73670, 20412, 114896, 5373, 100017, 5373, 71356, 20002, 57191, 99605, 104564, 3837, 104468, 20412, 89047, 5373, 117648, 105504, 71356, 64064, 9370, 101044, 1773, 104367, 114896, 123930, 239, 89047, 5373, 104992, 101044, 5373, 71356, 101044, 3837, 91676, 103926, 99283, 102157, 71356, 46477, 9370, 101044, 100132, 71356, 92374, 1773, 101908, 71356, 99486, 67071, 43288, 99454, 100694, 42140, 9370, 71356, 92374, 107339, 3837, 99360, 100694, 9370, 71356, 92374, 11622, 104516, 104634, 64064, 99793, 3837, 101894, 102495, 111867, 100145, 3837, 104301, 104564, 71356, 100786, 102498, 1773, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n","output_type":"stream"}]},{"cell_type":"code","source":"#使用BERT模型\n# model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\",num_labels=5)\n# print(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#【加载模型】\nid2label = {0: \"human\", 1: \"chatgpt\"}\nlabel2id = {\"human\": 0, \"chatgpt\": 1}\n#使用Qwen1.5模型\nmodel = AutoModelForSequenceClassification.from_pretrained(\"Qwen/Qwen1.5-0.5B\",num_labels=2,id2label=id2label,label2id=label2id)\nmodel.config.pad_token_id=model.config.eos_token_id #这里也要指定一下pad_token_id，不然训练时会报错 \"ValueError: Cannot handle batch sizes > 1 if no padding token is defined.\"\nprint(\"【model】\\n\",model)\nprint(\"【model.config】\\n\",model.config)\nprint(\"【model.config.pad_token_id】\",model.config.pad_token_id)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T04:33:58.480388Z","iopub.execute_input":"2024-06-17T04:33:58.480750Z","iopub.status.idle":"2024-06-17T04:34:04.025769Z","shell.execute_reply.started":"2024-06-17T04:33:58.480722Z","shell.execute_reply":"2024-06-17T04:34:04.024840Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/661 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb10cf5b2a1e4beea1ca0c1b47dcdc85"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.24G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7e298cebe41493ca0b9fabece58c4e9"}},"metadata":{}},{"name":"stderr","text":"Some weights of Qwen2ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen1.5-0.5B and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"【model】\n Qwen2ForSequenceClassification(\n  (model): Qwen2Model(\n    (embed_tokens): Embedding(151936, 1024)\n    (layers): ModuleList(\n      (0-23): 24 x Qwen2DecoderLayer(\n        (self_attn): Qwen2SdpaAttention(\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (o_proj): Linear(in_features=1024, out_features=1024, bias=False)\n          (rotary_emb): Qwen2RotaryEmbedding()\n        )\n        (mlp): Qwen2MLP(\n          (gate_proj): Linear(in_features=1024, out_features=2816, bias=False)\n          (up_proj): Linear(in_features=1024, out_features=2816, bias=False)\n          (down_proj): Linear(in_features=2816, out_features=1024, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): Qwen2RMSNorm()\n        (post_attention_layernorm): Qwen2RMSNorm()\n      )\n    )\n    (norm): Qwen2RMSNorm()\n  )\n  (score): Linear(in_features=1024, out_features=2, bias=False)\n)\n【model.config】\n Qwen2Config {\n  \"_name_or_path\": \"Qwen/Qwen1.5-0.5B\",\n  \"architectures\": [\n    \"Qwen2ForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151643,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 1024,\n  \"id2label\": {\n    \"0\": \"human\",\n    \"1\": \"chatgpt\"\n  },\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 2816,\n  \"label2id\": {\n    \"chatgpt\": 1,\n    \"human\": 0\n  },\n  \"max_position_embeddings\": 32768,\n  \"max_window_layers\": 21,\n  \"model_type\": \"qwen2\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 24,\n  \"num_key_value_heads\": 16,\n  \"pad_token_id\": 151643,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": 32768,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.41.2\",\n  \"use_cache\": true,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 151936\n}\n\n【model.config.pad_token_id】 151643\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 【训练】","metadata":{}},{"cell_type":"code","source":"#【训练参数】\nfrom datasets import load_metric\nimport numpy as np\n\ntraining_args = TrainingArguments(\n    output_dir=\"pt_save_pretrained\",\n    evaluation_strategy=\"epoch\", #每跑完一个epoch输出一下测试信息\n    num_train_epochs=2,\n    per_device_train_batch_size=4, # 一共要跑 len(dataset)/batch_size * epoch 个step\n                                  # [模型=Qwen1.5-0.5B, batch_size=4]：完全微调显存13.3GB，LoRA微调显存8.7GB\n#     gradient_accumulation_steps=2,\n    #     load_best_model_at_end=True,\n    save_strategy=\"no\",  #关闭自动保存模型（Kaggle上磁盘空间不太够）\n#     save_total_limit=1, #保存检查点数量的限制\n)\n\nmetric=load_metric('accuracy') #评估指标\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)\n\ndef get_trainer(model): \n    return  Trainer( \n        model=model, \n        args=training_args, \n        tokenizer=tokenizer,\n        train_dataset=train_dataset, \n        eval_dataset=eval_dataset, \n        compute_metrics=compute_metrics,\n        data_collator=DataCollatorWithPadding(tokenizer=tokenizer, padding=True,return_tensors=\"pt\"), #给数据添加padding弄成batch\n    ) ","metadata":{"execution":{"iopub.status.busy":"2024-06-17T04:36:59.615082Z","iopub.execute_input":"2024-06-17T04:36:59.615974Z","iopub.status.idle":"2024-06-17T04:37:00.187197Z","shell.execute_reply.started":"2024-06-17T04:36:59.615928Z","shell.execute_reply":"2024-06-17T04:37:00.186366Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/tmp/ipykernel_34/3640500368.py:17: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n  metric=load_metric('accuracy') #评估指标\n/opt/conda/lib/python3.10/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/1.65k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8921a053627c48408b6c423b05c9ee0a"}},"metadata":{}}]},{"cell_type":"code","source":"#【完全微调】\nprint(\"【开始训练】\")\ntrainer=get_trainer(model)\ntrainer.train()\n\ntokenizer.save_pretrained(\"./full_model_tokenizer\") \nmodel.save_pretrained(\"./full_model\")\n\n#Kaggle注意：\n#每次训练之后restart以释放显存！\n#factory也reset一下，不然磁盘空间会爆！","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#【PEFT-LoRA微调】\nfrom peft import LoraConfig, get_peft_model\n\npeft_config = LoraConfig(\n    task_type=\"SEQ_CLS\", #任务类型：分类 \n    target_modules=[\"q_proh\",\"k_proj\",\"v_proj\",\"o_proj\"],  # 这个不同的模型需要设置不同的参数，主要看模型中的attention层\n    inference_mode=False, # 关闭推理模式 (即开启训练模式)\n    r=8, # Lora 秩\n    lora_alpha=16, # Lora alaph，具体作用参见 Lora 原理\n    lora_dropout=0.1 # Dropout 比例\n)\n\npeft_model = get_peft_model(model, peft_config) # 加载lora参数peft框架\n\nprint('PEFT参数量：') \npeft_model.print_trainable_parameters() \n\nprint(\"【开始训练】\")\npeft_trainer=get_trainer(peft_model)\npeft_trainer.train()\n\ntokenizer.save_pretrained(\"./peft_model_tokenizer\") \npeft_model.save_pretrained(\"./peft_model\")","metadata":{"execution":{"iopub.status.busy":"2024-06-17T04:37:06.149373Z","iopub.execute_input":"2024-06-17T04:37:06.150074Z","iopub.status.idle":"2024-06-17T05:01:11.990221Z","shell.execute_reply.started":"2024-06-17T04:37:06.150041Z","shell.execute_reply":"2024-06-17T05:01:11.989119Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"PEFT参数量：\ntrainable params: 1,181,696 || all params: 465,171,456 || trainable%: 0.2540\n【开始训练】\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.1 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240617_043743-e2nrmyuv</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/chenxingling/huggingface/runs/e2nrmyuv' target=\"_blank\">pt_save_pretrained</a></strong> to <a href='https://wandb.ai/chenxingling/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/chenxingling/huggingface' target=\"_blank\">https://wandb.ai/chenxingling/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/chenxingling/huggingface/runs/e2nrmyuv' target=\"_blank\">https://wandb.ai/chenxingling/huggingface/runs/e2nrmyuv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2000/2000 23:08, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.102500</td>\n      <td>0.113912</td>\n      <td>0.986000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.052600</td>\n      <td>0.107569</td>\n      <td>0.986000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 【测试】","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import DataCollatorWithPadding,AutoTokenizer,AutoModelForSequenceClassification\n\ndef classify(example,show): #对example进行预测\n    text=example[\"text\"]\n    label=example[\"label\"]\n#     print(\"【example】\",example)\n    inputs = tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\").to('cuda')\n#     print(\"【inputs】\",inputs)\n    with torch.no_grad(): \n        output = inference_model(**inputs) \n        pred = output.logits.argmax(dim=-1).item() \n    if show:\n        print(\"【预测{}!】Label: {}, Pred_Label: {}\\nText: {}\".format(\"正确\" if label==pred else \"错误\",id2label[label],id2label[pred],text))\n    else:\n        return pred,label\n\n# inference_model=model.to('cuda')\ntokenizer = AutoTokenizer.from_pretrained(\"./peft_model_tokenizer\")\ninference_model = AutoModelForSequenceClassification.from_pretrained(\"./peft_model\").to('cuda') #读取训练好的模型\nprint(\"【model】\\n\",inference_model)\nprint(\"【model.config】\\n\",inference_model.config)\nprint(\"【model.config.pad_token_id】\",inference_model.config.pad_token_id)\ndata_collator=DataCollatorWithPadding(tokenizer=tokenizer, padding=True,return_tensors=\"pt\")\n\nid2label = {0: \"human\", 1: \"chatgpt\"}\nlabel2id = {\"human\": 0, \"chatgpt\": 1}\n\nclassify(data_test[0],1) #随便测试一个数据","metadata":{"execution":{"iopub.status.busy":"2024-06-17T05:03:03.627674Z","iopub.execute_input":"2024-06-17T05:03:03.628303Z","iopub.status.idle":"2024-06-17T05:03:05.923012Z","shell.execute_reply.started":"2024-06-17T05:03:03.628272Z","shell.execute_reply":"2024-06-17T05:03:05.922103Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nSome weights of Qwen2ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen1.5-0.5B and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"【model】\n Qwen2ForSequenceClassification(\n  (model): Qwen2Model(\n    (embed_tokens): Embedding(151936, 1024)\n    (layers): ModuleList(\n      (0-23): 24 x Qwen2DecoderLayer(\n        (self_attn): Qwen2SdpaAttention(\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (k_proj): lora.Linear(\n            (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n            (lora_dropout): ModuleDict(\n              (default): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (default): Linear(in_features=1024, out_features=8, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (default): Linear(in_features=8, out_features=1024, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n          )\n          (v_proj): lora.Linear(\n            (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n            (lora_dropout): ModuleDict(\n              (default): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (default): Linear(in_features=1024, out_features=8, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (default): Linear(in_features=8, out_features=1024, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n          )\n          (o_proj): lora.Linear(\n            (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n            (lora_dropout): ModuleDict(\n              (default): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (default): Linear(in_features=1024, out_features=8, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (default): Linear(in_features=8, out_features=1024, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n          )\n          (rotary_emb): Qwen2RotaryEmbedding()\n        )\n        (mlp): Qwen2MLP(\n          (gate_proj): Linear(in_features=1024, out_features=2816, bias=False)\n          (up_proj): Linear(in_features=1024, out_features=2816, bias=False)\n          (down_proj): Linear(in_features=2816, out_features=1024, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): Qwen2RMSNorm()\n        (post_attention_layernorm): Qwen2RMSNorm()\n      )\n    )\n    (norm): Qwen2RMSNorm()\n  )\n  (score): ModulesToSaveWrapper(\n    (original_module): Linear(in_features=1024, out_features=2, bias=False)\n    (modules_to_save): ModuleDict(\n      (default): Linear(in_features=1024, out_features=2, bias=False)\n    )\n  )\n)\n【model.config】\n Qwen2Config {\n  \"_name_or_path\": \"Qwen/Qwen1.5-0.5B\",\n  \"architectures\": [\n    \"Qwen2ForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151643,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 1024,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 2816,\n  \"max_position_embeddings\": 32768,\n  \"max_window_layers\": 21,\n  \"model_type\": \"qwen2\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 24,\n  \"num_key_value_heads\": 16,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": 32768,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.41.2\",\n  \"use_cache\": true,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 151936\n}\n\n【model.config.pad_token_id】 None\n【预测正确!】Label: human, Pred_Label: human\nText: 硬盘接口是硬盘与主机系统间的连接部件，作用是在硬盘缓存和主机内存之间传输数据。不同的硬盘接口决定着硬盘与计算机之间的连接速度，在整个系统中，硬盘接口的优劣直接影响着程序运行快慢和系统性能好坏。\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_metric\nfrom tqdm import tqdm \nmetric=load_metric('accuracy')\n\nprint(\"【测试集】\",data_test)\ninference_model.eval() \nfor i,example in enumerate(tqdm(data_test)): \n    pred, label = classify(example,0)\n#     print(\"NO.{} 预测{}! Label: {}, Pred_Label: {}\".format(i,\"正确\" if label==pred else \"错误\",id2label[label],id2label[pred]))\n    metric.add(predictions=pred, references=label)\nprint(metric.compute()) ","metadata":{"execution":{"iopub.status.busy":"2024-06-17T05:03:11.890612Z","iopub.execute_input":"2024-06-17T05:03:11.890977Z","iopub.status.idle":"2024-06-17T05:03:29.582198Z","shell.execute_reply.started":"2024-06-17T05:03:11.890948Z","shell.execute_reply":"2024-06-17T05:03:29.581023Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"【测试集】 Dataset({\n    features: ['label', 'text'],\n    num_rows: 500\n})\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 500/500 [00:17<00:00, 29.07it/s]","output_type":"stream"},{"name":"stdout","text":"{'accuracy': 0.982}\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]}]}